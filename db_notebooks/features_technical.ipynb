{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "584c7871-4614-4b97-a680-00f304831eb7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Install Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e8fd76b6-ba72-4dfb-8f49-6b7795bc86cd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "!pip install ta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "721494a4-b5c8-4bfe-b89e-eb46626aaafb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "56c7ad50-35a9-4c7f-9ecf-a7332797eb0d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"/Workspace/Shared/lib/\")\n",
    "import os\n",
    "import logging\n",
    "import time\n",
    "import requests\n",
    "from datetime import datetime\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.utils import AnalysisException\n",
    "from pyspark.sql.functions import expr\n",
    "from params import get_env, get_catalog, get_schema, get_table\n",
    "from ta import * # add_all_ta_features\n",
    "from ta.utils import dropna\n",
    "import ta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b0fa30bf-c46f-48ca-a379-ff372fab1ce7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "97a08a2f-175c-4b30-b7a6-1965afbec5b2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s %(name)s [%(levelname)s] %(message)s\",\n",
    "    stream=sys.stdout,\n",
    "    force=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "56b37cb5-37c2-4ba6-b870-eb107160300c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ff6ec848-639c-4b5a-bb60-aa5b11810abb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "env, catalog_suffix = get_env()\n",
    "catalog = get_catalog()\n",
    "schema = get_schema()\n",
    "table = get_table()\n",
    "\n",
    "print(f\"env = {env}\")\n",
    "print(f\"catalog_suffix = {catalog_suffix}\")\n",
    "print(f\"catalog = {catalog}\")\n",
    "print(f\"schema = {schema}\")\n",
    "print(f\"table = {table}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "415460bd-85b7-4cdf-b68e-802a882482d8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Single Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ec436031-d079-4a6f-b4d2-023b821ffc5e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_nvda = spark.sql(f\"select * FROM featlib{catalog_suffix}.components.pricing where act_symbol = 'NVDA' order by date\")\n",
    "df_nvda_pd = df_nvda.toPandas()\n",
    "\n",
    "# Add ta features filling NaN values\n",
    "df_nvda_pd_feat = add_all_ta_features(df_nvda_pd, open=\"open\", high=\"high\", low=\"low\", close=\"close\", volume=\"volume\", fillna=False)\\\n",
    "    .drop(['open', 'high', 'low', 'close', 'volume'], axis=1)\n",
    "\n",
    "df_nvda_pd_feat.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e79c3b7b-f51d-4aaa-a25d-f79abd99e197",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_nvda_pd_feat_melted = df_nvda_pd_feat.melt(id_vars=[\"date\", \"act_symbol\"], var_name=\"id\", value_name=\"values\")\n",
    "display(df_nvda_pd_feat_melted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6266b466-6808-4006-9850-59cac06fd8e8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_nvda_pd_feat_spark = spark.createDataFrame(df_nvda_pd_feat)\n",
    "display(df_nvda_pd_feat_spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9794c786-714b-4fce-ad98-bcd881ed3429",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "schema_str = \", \".join([f\"{field.name} {field.dataType.simpleString()}\" for field in df_nvda_pd_feat_spark.schema.fields])\n",
    "schema = f\"schema=\\\"{schema_str}\\\"\"\n",
    "schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d823c332-b4c7-44c3-9efd-f633591a2105",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import expr\n",
    "\n",
    "# Get a list of the columns you want to unpivot\n",
    "# Exclude 'date' and 'act_symbol' as they are your ID columns\n",
    "columns_to_unpivot = [col for col in df_nvda_pd_feat_spark.columns if col not in [\"date\", \"act_symbol\"]]\n",
    "\n",
    "# Construct the 'stack' expression dynamically\n",
    "# The format is 'number_of_columns, \"col1_name\", col1_value, \"col2_name\", col2_value, ...'\n",
    "stack_expression = \", \".join([f\"'{col}', {col}\" for col in columns_to_unpivot])\n",
    "num_columns = len(columns_to_unpivot)\n",
    "\n",
    "# Apply the stack function\n",
    "df_nvda_pd_feat_spark_pivoted = df_nvda_pd_feat_spark.selectExpr(\n",
    "    \"date\",\n",
    "    \"act_symbol\",\n",
    "    f\"stack({num_columns}, {stack_expression}) as (id, value)\"\n",
    ")\n",
    "\n",
    "display(df_nvda_pd_feat_spark_pivoted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "33f5cfcf-1779-4fc6-91da-00812749a952",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Create Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7474829f-779c-4e36-8f7f-a35c153061a6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Define query\n",
    "query = f\"select * FROM featlib{catalog_suffix}.components.pricing order by act_symbol, date\"\n",
    "\n",
    "# Read data from table\n",
    "df = spark.sql(query)\n",
    "\n",
    "# Convert to Pandas DataFrame\n",
    "df_pd = df.toPandas()\n",
    "\n",
    "# def process_symbol_group(group):\n",
    "#     return add_all_ta_features(\n",
    "#         group,\n",
    "#         open=\"open\",\n",
    "#         high=\"high\",\n",
    "#         low=\"low\",\n",
    "#         close=\"close\",\n",
    "#         volume=\"volume\",\n",
    "#         fillna=False\n",
    "#     )\n",
    "\n",
    "# # Apply the function only to groups with at least 50 rows\n",
    "# df_pd_feat = (\n",
    "#     df_pd.groupby(\"act_symbol\", group_keys=False)\n",
    "#     .apply(process_symbol_group)\n",
    "#     .reset_index(drop=True)\n",
    "# )\n",
    "\n",
    "# Add ta features filling NaN values\n",
    "# df_pd_feat = add_all_ta_features(df_pd, open=\"open\", high=\"high\", low=\"low\", close=\"close\", volume=\"volume\", fillna=False)\\\n",
    "#     .drop(['open', 'high', 'low', 'close', 'volume'], axis=1)\n",
    "\n",
    "# df_pd_wr = ta.momentum.WilliamsRIndicator(df_pd, low=\"low\", close=\"close\", fillna=False)\\\n",
    "#     .drop(['open', 'high', 'low', 'close', 'volume'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1cf4c86c-2271-4e49-aa63-be0712c6b128",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "55f8e2e6-9bc4-42d9-abe7-c8ea84ff7296",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def process_symbol_group(group):\n",
    "    return ta.momentum.WilliamsRIndicator(\n",
    "        high=df_pd[\"high\"],\n",
    "        low=df_pd[\"low\"],\n",
    "        close=df_pd[\"close\"],\n",
    "        lbp=14,\n",
    "        fillna=False\n",
    "    ).williams_r()\n",
    "\n",
    "# Apply the function only to groups with at least 50 rows\n",
    "df_pd_wr = (\n",
    "    df_pd.groupby(\"act_symbol\", group_keys=False)\n",
    "    .apply(process_symbol_group)\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "df_pd_wr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "951d0d61-7d8f-44ba-be42-ae10e438c45b",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": null,
       "filterBlob": "{\"version\":1,\"filterGroups\":[{\"enabled\":true,\"filterGroupId\":\"fg_bd162141\",\"op\":\"OR\",\"filters\":[{\"filterId\":\"f_a2b38b4f\",\"enabled\":true,\"columnId\":\"act_symbol\",\"dataType\":\"string\",\"filterType\":\"oneof\"}],\"local\":false,\"updatedAt\":1751282471937},{\"enabled\":true,\"filterGroupId\":\"fg_1d1c514\",\"op\":\"OR\",\"filters\":[{\"filterId\":\"f_3b582f8b\",\"enabled\":true,\"columnId\":\"act_symbol\",\"dataType\":\"string\",\"filterType\":\"oneof\",\"filterValues\":[\"ABCS\"],\"filterConfig\":{\"caseSensitive\":true}}],\"local\":false,\"updatedAt\":1751282486473}],\"syncTimestamp\":1751282486473}",
       "queryPlanFiltersBlob": "[{\"kind\":\"call\",\"function\":\"and\",\"args\":[{\"kind\":\"call\",\"function\":\"or\",\"args\":[{\"kind\":\"call\",\"function\":\"in\",\"args\":[{\"kind\":\"identifier\",\"identifier\":\"act_symbol\"},{\"kind\":\"literal\",\"value\":\"ABCS\",\"type\":\"string\"}]}]}]}]",
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import pandas_udf\n",
    "import pandas as pd\n",
    "import ta\n",
    "\n",
    "def process_symbol_group(group: pd.DataFrame) -> pd.DataFrame:\n",
    "    group['williams_r'] = ta.momentum.WilliamsRIndicator(\n",
    "        high=group[\"high\"],\n",
    "        low=group[\"low\"],\n",
    "        close=group[\"close\"],\n",
    "        lbp=14,\n",
    "        fillna=False\n",
    "    ).williams_r()\n",
    "    return group[['date', 'act_symbol', 'williams_r']]\n",
    "\n",
    "df_spark_wr = df.groupby(\"act_symbol\").applyInPandas(\n",
    "    process_symbol_group,\n",
    "    schema=\"date date, act_symbol string, williams_r double\"\n",
    ")\n",
    "display(df_spark_wr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a44a074c-057a-4df8-b8d2-09868529e569",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": null,
       "filterBlob": "{\"version\":1,\"filterGroups\":[],\"syncTimestamp\":1751286047641}",
       "queryPlanFiltersBlob": "[]",
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import pandas_udf\n",
    "import pandas as pd\n",
    "import ta\n",
    "\n",
    "def process_symbol_group(group: pd.DataFrame) -> pd.DataFrame:\n",
    "    # Ensure the group DataFrame has the correct column names\n",
    "    group.columns = [\"date\", \"act_symbol\", \"open\", \"high\", \"low\", \"close\", \"volume\"]\n",
    "    \n",
    "    return ta.add_all_ta_features(\n",
    "        group, open=\"open\", high=\"high\", low=\"low\", close=\"close\", volume=\"volume\", fillna=False\n",
    "    ).drop(['open', 'high', 'low', 'close', 'volume'], axis=1)\n",
    "\n",
    "df_spark_ta = df.groupby(\"act_symbol\").applyInPandas(\n",
    "    process_symbol_group,\n",
    "    schema=\"date date, act_symbol string, volume_adi double, volume_obv double, volume_cmf double, volume_fi double, volume_em double, volume_sma_em double, volume_vpt double, volume_vwap double, volume_mfi double, volume_nvi double, volatility_bbm double, volatility_bbh double, volatility_bbl double, volatility_bbw double, volatility_bbp double, volatility_bbhi double, volatility_bbli double, volatility_kcc double, volatility_kch double, volatility_kcl double, volatility_kcw double, volatility_kcp double, volatility_kchi double, volatility_kcli double, volatility_dcl double, volatility_dch double, volatility_dcm double, volatility_dcw double, volatility_dcp double, volatility_atr double, volatility_ui double, trend_macd double, trend_macd_signal double, trend_macd_diff double, trend_sma_fast double, trend_sma_slow double, trend_ema_fast double, trend_ema_slow double, trend_vortex_ind_pos double, trend_vortex_ind_neg double, trend_vortex_ind_diff double, trend_trix double, trend_mass_index double, trend_dpo double, trend_kst double, trend_kst_sig double, trend_kst_diff double, trend_ichimoku_conv double, trend_ichimoku_base double, trend_ichimoku_a double, trend_ichimoku_b double, trend_stc double, trend_adx double, trend_adx_pos double, trend_adx_neg double, trend_cci double, trend_visual_ichimoku_a double, trend_visual_ichimoku_b double, trend_aroon_up double, trend_aroon_down double, trend_aroon_ind double, trend_psar_up double, trend_psar_down double, trend_psar_up_indicator double, trend_psar_down_indicator double, momentum_rsi double, momentum_stoch_rsi double, momentum_stoch_rsi_k double, momentum_stoch_rsi_d double, momentum_tsi double, momentum_uo double, momentum_stoch double, momentum_stoch_signal double, momentum_wr double, momentum_ao double, momentum_roc double, momentum_ppo double, momentum_ppo_signal double, momentum_ppo_hist double, momentum_pvo double, momentum_pvo_signal double, momentum_pvo_hist double, momentum_kama double, others_dr double, others_dlr double, others_cr double\"\n",
    ")\n",
    "display(df_spark_ta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2a50a5af-8105-4f92-95c1-7e121f2b3f0e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_spark_ta.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b18471d0-08d2-4397-9979-620d59594a1d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_nvda = df_spark_wr.filter(df_spark_wr[\"act_symbol\"] == \"NVDA\")\n",
    "\n",
    "display(df_nvda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3fe75e94-e22f-4b63-8f4d-c1570768f4d0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "comparison_df = df_nvda.join(df_nvda_pd_feat_spark, on=[\"date\", \"act_symbol\"], how=\"inner\")\n",
    "\n",
    "display(comparison_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d7beab59-1f24-4fce-9a28-b4f94e955cd2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_spark_ta.write.saveAsTable(\"featlib_dev.features.ta_raw\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "78fa26e2-17a1-48f5-b226-e95caa36b208",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_pd_wr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "10184096-4932-42d9-b8c6-8dc70fd23422",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0565bd66-e483-47ad-8e98-44ec8990e84c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Convert to Spark DataFrame\n",
    "df_feat = spark.createDataFrame(df_pd_feat)\n",
    "\n",
    "# Display\n",
    "df_feat.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a90163a1-18f6-404c-a1f3-1f29c1151d48",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Unpivot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f8ab044e-3239-44fa-bc41-f7eb12c895b1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Get a list of the columns you want to unpivot\n",
    "# Exclude 'date' and 'act_symbol' as they are ID columns\n",
    "columns_to_unpivot = [col for col in df_feat.columns if col not in [\"date\", \"act_symbol\"]]\n",
    "\n",
    "# Construct the 'stack' expression dynamically\n",
    "# The format is 'number_of_columns, \"col1_name\", col1_value, \"col2_name\", col2_value, ...'\n",
    "stack_expression = \", \".join([f\"'{col}', {col}\" for col in columns_to_unpivot])\n",
    "num_columns = len(columns_to_unpivot)\n",
    "\n",
    "# Apply the stack function\n",
    "df_feat_unpivot = df_feat.selectExpr(\n",
    "    \"date\",\n",
    "    \"act_symbol\",\n",
    "    f\"stack({num_columns}, {stack_expression}) as (id, value)\"\n",
    ")\n",
    "\n",
    "# Display\n",
    "df_feat_unpivot.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "47c0ecc4-3c14-47ed-81b6-94a111399992",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Save df_feat_unpivot to the specified table\n",
    "df_feat_unpivot.write.mode(\"overwrite\").saveAsTable(f\"featlib{catalog_suffix}.features.ta_raw\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": "HIGH"
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 7248815710195929,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "features_technical",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
