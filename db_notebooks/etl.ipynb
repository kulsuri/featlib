{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "721494a4-b5c8-4bfe-b89e-eb46626aaafb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "56c7ad50-35a9-4c7f-9ecf-a7332797eb0d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append(\"/Workspace/Shared/lib/\")\n",
    "import os\n",
    "import time\n",
    "import requests\n",
    "from datetime import datetime\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.utils import AnalysisException\n",
    "from params import get_env, get_catalog, get_schema, get_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "33f5cfcf-1779-4fc6-91da-00812749a952",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# ETL Object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "541e6033-3bf9-4350-95b4-05e03b67df0b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "class CsvDownloaderAndTableCreator:\n",
    "    \"\"\"\n",
    "    A class to download a CSV file, save it to a specified location with a timestamp suffix,\n",
    "    and create or replace a table in Databricks.\n",
    "\n",
    "    Attributes:\n",
    "        url (str): The URL to download the CSV file from.\n",
    "        save_location (str): The base location to save the downloaded CSV file.\n",
    "        table_name (str): The name of the table to create or replace in Databricks.\n",
    "        retries (int): The number of retry attempts if the download fails. Default is 3.\n",
    "        spark (SparkSession): A Spark session to interact with Databricks.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, url: str, save_location: str, table_name: str, retries: int = 3):\n",
    "        \"\"\"\n",
    "        Initializes the CsvDownloaderAndTableCreator with URL, save location, table name, and retry settings.\n",
    "\n",
    "        Args:\n",
    "            url (str): URL of the CSV file to download.\n",
    "            save_location (str): Local path to save the CSV file.\n",
    "            table_name (str): The name of the Databricks table to create or replace.\n",
    "            retries (int, optional): Number of retries for the file download. Default is 3.\n",
    "        \"\"\"\n",
    "        self.url = url\n",
    "        self.save_location = save_location\n",
    "        self.table_name = table_name\n",
    "        self.retries = retries\n",
    "        self.spark = SparkSession.builder.appName(\n",
    "            \"CsvDownloaderAndTableCreator\"\n",
    "        ).getOrCreate()\n",
    "\n",
    "    def _generate_filename_with_timestamp(self):\n",
    "        \"\"\"\n",
    "        Generates a filename with the last word of the table_name and a timestamp suffix\n",
    "        to prevent overwriting of files.\n",
    "\n",
    "        Returns:\n",
    "            str: The new filename with a timestamp suffix.\n",
    "        \"\"\"\n",
    "        # Get the last part of the table_name after the last dot (e.g., 'dividend' from 'dolt.stocks.dividend')\n",
    "        table_name_last_word = self.table_name.split(\".\")[-1]\n",
    "\n",
    "        # Generate a timestamp and combine it with the last word of the table name\n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        filename = f\"{table_name_last_word}_{timestamp}.csv\"\n",
    "\n",
    "        return os.path.join(os.path.dirname(self.save_location), filename)\n",
    "\n",
    "    def download_csv(self):\n",
    "        \"\"\"\n",
    "        Downloads the CSV file from the URL and saves it to the specified location.\n",
    "\n",
    "        Retries the download up to a specified number of times in case of failure.\n",
    "\n",
    "        Raises:\n",
    "            Exception: If the file cannot be downloaded after the specified number of retries.\n",
    "        \"\"\"\n",
    "        attempt = 0\n",
    "        # Generate a unique filename with timestamp to prevent overwriting\n",
    "        unique_filename = self._generate_filename_with_timestamp()\n",
    "\n",
    "        while attempt < self.retries:\n",
    "            try:\n",
    "                # Send HTTP request to get the CSV file\n",
    "                response = requests.get(self.url)\n",
    "                response.raise_for_status()  # Check if the request was successful\n",
    "\n",
    "                # Save the CSV content to the specified file path\n",
    "                with open(unique_filename, \"wb\") as file:\n",
    "                    file.write(response.content)\n",
    "                print(f\"File successfully downloaded to {unique_filename}\")\n",
    "                return unique_filename  # Exit the function if download is successful\n",
    "            except requests.exceptions.RequestException as e:\n",
    "                attempt += 1\n",
    "                print(f\"Attempt {attempt} failed: {e}\")\n",
    "                if attempt < self.retries:\n",
    "                    print(\"Retrying...\")\n",
    "                    time.sleep(2)  # Wait for 2 seconds before retrying\n",
    "                else:\n",
    "                    raise Exception(\n",
    "                        f\"Failed to download the CSV file after {self.retries} attempts.\"\n",
    "                    ) from e\n",
    "\n",
    "    def create_or_replace_table(self, csv_file_path: str):\n",
    "        \"\"\"\n",
    "        Creates or replaces a table in Databricks using the downloaded CSV file.\n",
    "\n",
    "        This method assumes the CSV file is in the correct format for creating a table.\n",
    "\n",
    "        Args:\n",
    "            csv_file_path (str): The path of the downloaded CSV file to create the table from.\n",
    "\n",
    "        Raises:\n",
    "            AnalysisException: If there is an error when creating or replacing the table.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Read the CSV file into a DataFrame\n",
    "            df = self.spark.read.option(\"header\", \"true\").csv(csv_file_path)\n",
    "\n",
    "            # Create or replace the table in Databricks\n",
    "            df.createOrReplaceTempView(self.table_name)\n",
    "            print(\n",
    "                f\"Table {self.table_name} created or replaced successfully in Databricks.\"\n",
    "            )\n",
    "        except AnalysisException as e:\n",
    "            print(f\"Error while creating or replacing the table: {e}\")\n",
    "            raise\n",
    "\n",
    "    def execute(self):\n",
    "        \"\"\"\n",
    "        Executes the entire process: downloading the CSV file and creating or replacing the table in Databricks.\n",
    "\n",
    "        First, it attempts to download the CSV file. Then, it creates or replaces the table in Databricks.\n",
    "        \"\"\"\n",
    "        # Download the CSV file and get the file path\n",
    "        downloaded_csv_path = self.download_csv()\n",
    "\n",
    "        # Create or replace the table in Databricks using the downloaded file\n",
    "        self.create_or_replace_table(downloaded_csv_path)\n",
    "\n",
    "\n",
    "# # Example usage:\n",
    "# if __name__ == \"__main__\":\n",
    "#     # Define the parameters\n",
    "#     url = \"https://example.com/stock_prices.csv\"\n",
    "#     save_location = \"/dbfs/mnt/your-volume/stock_prices.csv\"\n",
    "#     table_name = \"dolt.stocks.dividend\"\n",
    "\n",
    "#     # Create an instance of CsvDownloaderAndTableCreator\n",
    "#     csv_downloader = CsvDownloaderAndTableCreator(url, save_location, table_name)\n",
    "\n",
    "#     # Execute the process\n",
    "#     csv_downloader.execute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9dab8e3d-72a0-4d38-9366-fb0e3299264c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dividend_url = \"https://www.dolthub.com/csv/post-no-preference/stocks/post-no-preference%2Fdocs-belligerent-armadillo/dividend?include_bom=0\"\n",
    "div_path = \"/Volumes/dolt/stocks/raw/dividend/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "73f4dd1e-44c5-4b7b-997c-9e7973657eff",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "etl_dividend = CsvDownloaderAndTableCreator(\n",
    "    url=dividend_url, save_location=div_path, table_name=\"dolt.stocks.dividend\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "770da744-30c8-467a-affb-14aa72c4ebb6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "etl_dividend.download_csv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6727af92-630b-4ff7-9b99-77be07addec0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "etl_dividend.execute()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "etl",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
