{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "721494a4-b5c8-4bfe-b89e-eb46626aaafb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "56c7ad50-35a9-4c7f-9ecf-a7332797eb0d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append(\"/Workspace/Shared/lib/\")\n",
    "import os\n",
    "import logging\n",
    "import time\n",
    "import requests\n",
    "from datetime import datetime\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.utils import AnalysisException\n",
    "from params import get_env, get_catalog, get_schema, get_table, get_url, get_volume"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b0fa30bf-c46f-48ca-a379-ff372fab1ce7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "97a08a2f-175c-4b30-b7a6-1965afbec5b2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s %(name)s [%(levelname)s] %(message)s\",\n",
    "    stream=sys.stdout,\n",
    "    force=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "56b37cb5-37c2-4ba6-b870-eb107160300c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ff6ec848-639c-4b5a-bb60-aa5b11810abb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "env, catalog_suffix = get_env()\n",
    "catalog = get_catalog()\n",
    "schema = get_schema()\n",
    "table = get_table()\n",
    "url = get_url()\n",
    "volume = get_volume()\n",
    "\n",
    "# # Temporary override for testing\n",
    "# catalog = \"dolt\" + catalog_suffix\n",
    "# schema = \"stocks\"\n",
    "# table = \"symbol\"\n",
    "# url = \"https://www.dolthub.com/csv/post-no-preference/stocks/post-no-preference%2Fdocs-belligerent-armadillo/dividend?include_bom=0\"\n",
    "# url = \"https://www.dolthub.com/csv/post-no-preference/stocks/post-no-preference%2Fdocs-belligerent-armadillo/symbol?include_bom=0\"\n",
    "# volume = f\"/Volumes/{catalog}/stocks/raw/symbol/\"\n",
    "\n",
    "print(f\"env = {env}\")\n",
    "print(f\"catalog_suffix = {catalog_suffix}\")\n",
    "print(f\"catalog = {catalog}\")\n",
    "print(f\"schema = {schema}\")\n",
    "print(f\"table = {table}\")\n",
    "print(f\"url = {url}\")\n",
    "print(f\"volume = {volume}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "33f5cfcf-1779-4fc6-91da-00812749a952",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# ETL Object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "541e6033-3bf9-4350-95b4-05e03b67df0b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "class CsvDownloaderAndTableCreator:\n",
    "    \"\"\"\n",
    "    A class to download a CSV file, save it to a specified location with a timestamp suffix,\n",
    "    and create or replace a table in Databricks.\n",
    "\n",
    "    Attributes:\n",
    "        url (str): The URL to download the CSV file from.\n",
    "        save_location (str): The base location to save the downloaded CSV file.\n",
    "        table_name (str): The name of the table to create or replace in Databricks.\n",
    "        retries (int): The number of retry attempts if the download fails. Default is 3.\n",
    "        spark (SparkSession): A Spark session to interact with Databricks.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, url: str, save_location: str, table_name: str, retries: int = 3):\n",
    "        \"\"\"\n",
    "        Initializes the CsvDownloaderAndTableCreator with URL, save location, table name, and retry settings.\n",
    "\n",
    "        Args:\n",
    "            url (str): URL of the CSV file to download.\n",
    "            save_location (str): Local path to save the CSV file.\n",
    "            table_name (str): The name of the Databricks table to create or replace.\n",
    "            retries (int, optional): Number of retries for the file download. Default is 3.\n",
    "        \"\"\"\n",
    "        self.url = url\n",
    "        self.save_location = save_location\n",
    "        self.table_name = table_name\n",
    "        self.retries = retries\n",
    "        self.spark = SparkSession.builder.appName(\n",
    "            \"CsvDownloaderAndTableCreator\"\n",
    "        ).getOrCreate()\n",
    "\n",
    "    def _generate_filename_with_timestamp(self):\n",
    "        \"\"\"\n",
    "        Generates a filename with the last word of the table_name and a timestamp suffix\n",
    "        to prevent overwriting of files.\n",
    "\n",
    "        Returns:\n",
    "            str: The new filename with a timestamp suffix.\n",
    "        \"\"\"\n",
    "        # Get the last part of the table_name after the last dot (e.g., 'dividend' from 'dolt.stocks.dividend')\n",
    "        table_name_last_word = self.table_name.split(\".\")[-1]\n",
    "\n",
    "        # Generate a timestamp and combine it with the last word of the table name\n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        filename = f\"{table_name_last_word}_{timestamp}.csv\"\n",
    "\n",
    "        return os.path.join(os.path.dirname(self.save_location), filename)\n",
    "\n",
    "    def download_csv(self):\n",
    "        \"\"\"\n",
    "        Downloads the CSV file from the URL and saves it to the specified location.\n",
    "\n",
    "        Retries the download up to a specified number of times in case of failure.\n",
    "\n",
    "        Raises:\n",
    "            Exception: If the file cannot be downloaded after the specified number of retries.\n",
    "        \"\"\"\n",
    "        attempt = 0\n",
    "        # Generate a unique filename with timestamp to prevent overwriting\n",
    "        unique_filename = self._generate_filename_with_timestamp()\n",
    "\n",
    "        while attempt < self.retries:\n",
    "            try:\n",
    "                # Send HTTP request to get the CSV file\n",
    "                response = requests.get(self.url)\n",
    "                response.raise_for_status()  # Check if the request was successful\n",
    "\n",
    "                # Save the CSV content to the specified file path\n",
    "                with open(unique_filename, \"wb\") as file:\n",
    "                    file.write(response.content)\n",
    "                print(f\"File successfully downloaded to {unique_filename}\")\n",
    "                return unique_filename  # Exit the function if download is successful\n",
    "            except requests.exceptions.RequestException as e:\n",
    "                attempt += 1\n",
    "                print(f\"Attempt {attempt} failed: {e}\")\n",
    "                if attempt < self.retries:\n",
    "                    print(\"Retrying...\")\n",
    "                    time.sleep(2)  # Wait for 2 seconds before retrying\n",
    "                else:\n",
    "                    raise Exception(\n",
    "                        f\"Failed to download the CSV file after {self.retries} attempts.\"\n",
    "                    ) from e\n",
    "\n",
    "    def create_or_replace_table(self, csv_file_path: str):\n",
    "        \"\"\"\n",
    "        Creates or replaces a table in Databricks using the downloaded CSV file.\n",
    "\n",
    "        This method assumes the CSV file is in the correct format for creating a table.\n",
    "\n",
    "        Args:\n",
    "            csv_file_path (str): The path of the downloaded CSV file to create the table from.\n",
    "\n",
    "        Raises:\n",
    "            AnalysisException: If there is an error when creating or replacing the table.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Read the CSV file into a DataFrame\n",
    "            df = self.spark.read.option(\"header\", \"true\").csv(csv_file_path)\n",
    "\n",
    "            # Create or replace the table in Databricks\n",
    "            df.createOrReplaceTempView(self.table_name)\n",
    "            print(\n",
    "                f\"Table {self.table_name} created or replaced successfully in Databricks.\"\n",
    "            )\n",
    "        except AnalysisException as e:\n",
    "            print(f\"Error while creating or replacing the table: {e}\")\n",
    "            raise\n",
    "\n",
    "    def execute(self):\n",
    "        \"\"\"\n",
    "        Executes the entire process: downloading the CSV file and creating or replacing the table in Databricks.\n",
    "\n",
    "        First, it attempts to download the CSV file. Then, it creates or replaces the table in Databricks.\n",
    "        \"\"\"\n",
    "        # Download the CSV file and get the file path\n",
    "        downloaded_csv_path = self.download_csv()\n",
    "\n",
    "        # Create or replace the table in Databricks using the downloaded file\n",
    "        self.create_or_replace_table(downloaded_csv_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8918bea6-ccc3-41ae-afb1-ddb7ba94e545",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "class CsvDownloaderAndTableCreator:\n",
    "    \"\"\"\n",
    "    A class to download a CSV file, save it to a specified location with a timestamp suffix,\n",
    "    and create or replace a table in Databricks.\n",
    "\n",
    "    Attributes:\n",
    "        url (str): The URL to download the CSV file from.\n",
    "        save_location (str): The base location to save the downloaded CSV file.\n",
    "        table_name (str): The name of the table to create or replace in Databricks.\n",
    "        retries (int): The number of retry attempts if the download fails. Default is 3.\n",
    "        spark (SparkSession): A Spark session to interact with Databricks.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, url: str, save_location: str, table_name: str, retries: int = 3):\n",
    "        \"\"\"\n",
    "        Initializes the CsvDownloaderAndTableCreator with URL, save location, table name, and retry settings.\n",
    "\n",
    "        Args:\n",
    "            url (str): URL of the CSV file to download.\n",
    "            save_location (str): Local path to save the CSV file.\n",
    "            table_name (str): The name of the Databricks table to create or replace.\n",
    "            retries (int, optional): Number of retries for the file download. Default is 3.\n",
    "        \"\"\"\n",
    "        self.url = url\n",
    "        self.save_location = save_location\n",
    "        self.table_name = table_name\n",
    "        self.retries = retries\n",
    "        self.spark = SparkSession.builder.appName(\n",
    "            \"CsvDownloaderAndTableCreator\"\n",
    "        ).getOrCreate()\n",
    "\n",
    "    def _generate_filename_with_timestamp(self):\n",
    "        \"\"\"\n",
    "        Generates a filename with the last word of the table_name and a timestamp suffix\n",
    "        to prevent overwriting of files.\n",
    "\n",
    "        Returns:\n",
    "            str: The new filename with a timestamp suffix.\n",
    "        \"\"\"\n",
    "        # Get the last part of the table_name after the last dot (e.g., 'dividend' from 'dolt.stocks.dividend')\n",
    "        table_name_last_word = self.table_name.split(\".\")[-1]\n",
    "\n",
    "        # Generate a timestamp and combine it with the last word of the table name\n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        filename = f\"{table_name_last_word}_{timestamp}.csv\"\n",
    "\n",
    "        return os.path.join(os.path.dirname(self.save_location), filename)\n",
    "\n",
    "    def download_csv(self):\n",
    "        \"\"\"\n",
    "        Downloads the CSV file from the URL and saves it to the specified location.\n",
    "\n",
    "        Retries the download up to a specified number of times in case of failure.\n",
    "\n",
    "        Raises:\n",
    "            Exception: If the file cannot be downloaded after the specified number of retries.\n",
    "        \"\"\"\n",
    "        attempt = 0\n",
    "        # Generate a unique filename with timestamp to prevent overwriting\n",
    "        unique_filename = self._generate_filename_with_timestamp()\n",
    "\n",
    "        while attempt < self.retries:\n",
    "            try:\n",
    "                # Send HTTP request to get the CSV file\n",
    "                response = requests.get(self.url)\n",
    "                response.raise_for_status()  # Check if the request was successful\n",
    "\n",
    "                # Save the CSV content to the specified file path\n",
    "                with open(unique_filename, \"wb\") as file:\n",
    "                    file.write(response.content)\n",
    "\n",
    "                logging.info(f\"File successfully downloaded to {unique_filename}\")\n",
    "                return unique_filename  # Exit the function if download is successful\n",
    "            except requests.exceptions.RequestException as e:\n",
    "                attempt += 1\n",
    "                logging.warning(f\"Attempt {attempt} failed: {e}\")\n",
    "                if attempt < self.retries:\n",
    "                    logging.info(\"Retrying...\")\n",
    "                    time.sleep(2)  # Wait for 2 seconds before retrying\n",
    "                else:\n",
    "                    logging.error(\n",
    "                        f\"Failed to download the CSV file after {self.retries} attempts.\"\n",
    "                    )\n",
    "                    raise Exception(\n",
    "                        f\"Failed to download the CSV file after {self.retries} attempts.\"\n",
    "                    ) from e\n",
    "\n",
    "    def create_or_replace_table(self, csv_file_path: str):\n",
    "        \"\"\"\n",
    "        Creates or replaces a table in Databricks using the downloaded CSV file.\n",
    "\n",
    "        This method assumes the CSV file is in the correct format for creating a table.\n",
    "\n",
    "        Args:\n",
    "            csv_file_path (str): The path of the downloaded CSV file to create the table from.\n",
    "\n",
    "        Raises:\n",
    "            AnalysisException: If there is an error when creating or replacing the table.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Read the CSV file into a DataFrame\n",
    "            df = self.spark.read.option(\"header\", \"true\").csv(csv_file_path)\n",
    "\n",
    "            # Create or replace the table in Databricks\n",
    "            df.writeTo(self.table_name).createOrReplace()\n",
    "            logging.info(\n",
    "                f\"Table {self.table_name} created or replaced successfully in Databricks.\"\n",
    "            )\n",
    "        except AnalysisException as e:\n",
    "            logging.error(f\"Error while creating or replacing the table: {e}\")\n",
    "            raise\n",
    "\n",
    "    def execute(self):\n",
    "        \"\"\"\n",
    "        Executes the entire process: downloading the CSV file and creating or replacing the table in Databricks.\n",
    "\n",
    "        First, it attempts to download the CSV file. Then, it creates or replaces the table in Databricks.\n",
    "        \"\"\"\n",
    "        # Download the CSV file and get the file path\n",
    "        downloaded_csv_path = self.download_csv()\n",
    "\n",
    "        # Create or replace the table in Databricks using the downloaded file\n",
    "        self.create_or_replace_table(downloaded_csv_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "20709e6a-ea1c-42c1-9730-312a05a0d25c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Execute ETL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "73f4dd1e-44c5-4b7b-997c-9e7973657eff",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Create object\n",
    "etl = CsvDownloaderAndTableCreator(\n",
    "    url=url, save_location=volume, table_name=f\"{catalog}.{schema}.{table}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6727af92-630b-4ff7-9b99-77be07addec0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Execute download and table creation\n",
    "etl.execute()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 7248815710195929,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "etl",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
