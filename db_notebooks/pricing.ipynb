{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "721494a4-b5c8-4bfe-b89e-eb46626aaafb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "56c7ad50-35a9-4c7f-9ecf-a7332797eb0d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"/Workspace/Shared/lib/\")\n",
    "import os\n",
    "import logging\n",
    "import time\n",
    "import requests\n",
    "from datetime import datetime\n",
    "import plotly.express as px\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.utils import AnalysisException\n",
    "from params import get_env, get_catalog, get_schema, get_table, get_url, get_volume"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b0fa30bf-c46f-48ca-a379-ff372fab1ce7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "97a08a2f-175c-4b30-b7a6-1965afbec5b2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s %(name)s [%(levelname)s] %(message)s\",\n",
    "    stream=sys.stdout,\n",
    "    force=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "56b37cb5-37c2-4ba6-b870-eb107160300c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ff6ec848-639c-4b5a-bb60-aa5b11810abb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "env, catalog_suffix = get_env()\n",
    "catalog = get_catalog()\n",
    "schema = get_schema()\n",
    "table = get_table()\n",
    "url = get_url()\n",
    "volume = get_volume()\n",
    "\n",
    "print(f\"env = {env}\")\n",
    "print(f\"catalog_suffix = {catalog_suffix}\")\n",
    "print(f\"catalog = {catalog}\")\n",
    "print(f\"schema = {schema}\")\n",
    "print(f\"table = {table}\")\n",
    "print(f\"url = {url}\")\n",
    "print(f\"volume = {volume}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "33f5cfcf-1779-4fc6-91da-00812749a952",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Split Adjustment Factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8918bea6-ccc3-41ae-afb1-ddb7ba94e545",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.sql(\n",
    "    f\"\"\"\n",
    "    CREATE OR REPLACE TABLE {catalog}.{schema}.split_adj AS\n",
    "        -- Get the unique symbols from the split table\n",
    "        WITH base_table AS (\n",
    "            SELECT DISTINCT act_symbol FROM raw{catalog_suffix}.stocks.split\n",
    "        ),\n",
    "        -- Create a new table with a row for each symbol and a NULL split for date 2000-01-01\n",
    "        new_split_table AS (\n",
    "            SELECT \n",
    "                act_symbol,\n",
    "                DATE '2000-01-01' as ex_date,\n",
    "                NULL as to_factor,\n",
    "                NULL as for_factor\n",
    "            FROM base_table\n",
    "            UNION ALL\n",
    "            SELECT * FROM raw{catalog_suffix}.stocks.split\n",
    "        ),\n",
    "        -- Calculate the adjustment factor\n",
    "        split_factors AS (\n",
    "            -- Calculate adjustment factors and order the rows\n",
    "            SELECT\n",
    "                act_symbol,\n",
    "                ex_date AS adj_date,\n",
    "                try_divide(double(for_factor), double(to_factor)) AS adj_factor\n",
    "            FROM new_split_table\n",
    "        ),\n",
    "        -- Order the factors and assign end dates\n",
    "        ordered_factors AS (\n",
    "            -- Assign end dates and ensure chronological order\n",
    "            SELECT\n",
    "                act_symbol,\n",
    "                adj_date,\n",
    "                CAST (LEAD(adj_date) OVER (PARTITION BY act_symbol ORDER BY adj_date ASC) - INTERVAL '1 DAY' AS DATE) AS end_adj_date,\n",
    "                adj_factor,\n",
    "                ROW_NUMBER() OVER (PARTITION BY act_symbol ORDER BY adj_date DESC) AS reverse_row_num\n",
    "            FROM split_factors\n",
    "        ),\n",
    "        -- Calculate the cumulative adjustment factor\n",
    "        cumulative_factors AS (\n",
    "            -- Compute cumulative adjustment factors in reverse order\n",
    "            SELECT\n",
    "                act_symbol,\n",
    "                adj_date,\n",
    "                end_adj_date,\n",
    "                adj_factor,\n",
    "                -- Reverse chronological cumulative multiplication\n",
    "                CASE \n",
    "                    WHEN end_adj_date IS NULL \n",
    "                        THEN 1\n",
    "                    WHEN lead(end_adj_date) OVER (PARTITION BY act_symbol ORDER BY adj_date ASC) IS NULL\n",
    "                        THEN lead(adj_factor) OVER (PARTITION BY act_symbol ORDER BY adj_date ASC)\n",
    "                    --ELSE product(adj_factor) OVER (PARTITION BY act_symbol ORDER BY reverse_row_num ASC ROWS BETWEEN UNBOUNDED PRECEDING AND 1 PRECEDING)\n",
    "                    ELSE EXP(SUM(LN(adj_factor)) OVER (PARTITION BY act_symbol ORDER BY reverse_row_num ASC ROWS BETWEEN UNBOUNDED PRECEDING AND 1 PRECEDING))\n",
    "                END AS cum_adj_factor\n",
    "            FROM ordered_factors\n",
    "        )\n",
    "        -- Final output\n",
    "        SELECT\n",
    "            act_symbol,\n",
    "            adj_date,\n",
    "            end_adj_date,\n",
    "            adj_factor,\n",
    "            cum_adj_factor\n",
    "        FROM cumulative_factors\n",
    "        ORDER BY adj_date ASC;\n",
    "    \"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "20709e6a-ea1c-42c1-9730-312a05a0d25c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Pricing Inc. Splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "73f4dd1e-44c5-4b7b-997c-9e7973657eff",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.sql(\n",
    "    f\"\"\"\n",
    "    -- 17,457\n",
    "    CREATE OR REPLACE TEMPORARY VIEW tmp_pricing01_inc_splits AS (\n",
    "        with ohlcv_split as (\n",
    "            select\n",
    "                to_date(a.date) as date,\n",
    "                a.act_symbol,\n",
    "                a.open * COALESCE(f.cum_adj_factor, 1) as open,\n",
    "                a.high * COALESCE(f.cum_adj_factor, 1) as high,\n",
    "                a.low * COALESCE(f.cum_adj_factor, 1) as low,\n",
    "                a.close * COALESCE(f.cum_adj_factor, 1) as close,\n",
    "                a.volume / COALESCE(f.cum_adj_factor, 1) as volume\n",
    "            FROM raw{catalog_suffix}.stocks.ohlcv a\n",
    "            LEFT JOIN featlib{catalog_suffix}.components.split_adj f \n",
    "                ON a.act_symbol=f.act_symbol \n",
    "                AND a.date >= f.adj_date \n",
    "                AND (a.date <= f.end_adj_date OR f.end_adj_date IS NULL)\n",
    "        )\n",
    "        select * from ohlcv_split\n",
    "    )\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "spark.sql(\"select * from tmp_pricing01_inc_splits\").display()\n",
    "\n",
    "spark.sql(f\"with cte as (select distinct act_symbol from tmp_pricing01_inc_splits) select count(*) as unique_act_symbol from cte\").display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "821aec5a-f5fc-424d-b60d-6dd537c8c635",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Remove Symbols With History Gaps\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cf33f18c-9de5-4740-9a29-5064d5f18bf6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.sql(\n",
    "    f\"\"\"\n",
    "    -- 14,566\n",
    "    -- Remove symbols with gaps > 14 days\n",
    "    CREATE OR REPLACE TEMPORARY VIEW tmp_pricing02_remsym_misshist AS (\n",
    "        WITH day_difference AS (\n",
    "            SELECT\n",
    "                *,\n",
    "                int(LEAD(date) OVER (PARTITION BY act_symbol ORDER BY date) - date) AS day_count\n",
    "            FROM\n",
    "                tmp_pricing01_inc_splits\n",
    "            ),\n",
    "        filtered_symbols AS (\n",
    "            SELECT\n",
    "                act_symbol\n",
    "            FROM\n",
    "                day_difference\n",
    "            WHERE\n",
    "                day_count > 14\n",
    "            GROUP BY\n",
    "                act_symbol\n",
    "        )\n",
    "        SELECT\n",
    "            * except (day_count)\n",
    "        FROM\n",
    "            day_difference\n",
    "        WHERE\n",
    "            act_symbol NOT IN (SELECT act_symbol FROM filtered_symbols)\n",
    "        AND open IS NOT NULL \n",
    "        AND high IS NOT NULL \n",
    "        AND low IS NOT NULL \n",
    "        AND close IS NOT NULL \n",
    "        AND volume IS NOT NULL\n",
    "    )\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "spark.sql(\"select * from tmp_pricing02_remsym_misshist\").display()\n",
    "\n",
    "spark.sql(f\"with cte as (select distinct act_symbol from tmp_pricing02_remsym_misshist) select count(*) as unique_act_symbol from cte\").display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6936c42e-50bc-4356-9509-9818b2f7785d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Remove Zero Prices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4ebcf471-3e94-4674-9d25-327472135d44",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.sql(\n",
    "    \"\"\"\n",
    "    -- Remove symbols which have more than 5 zero values for ohlcv\n",
    "    -- Massage the data to replace zeros with the avg/min/max of the previous 5 days depending on the column\n",
    "    CREATE OR REPLACE TEMPORARY VIEW tmp_pricing03_remsym_zeros AS (\n",
    "        WITH zero_close_count AS (\n",
    "        SELECT\n",
    "            act_symbol,\n",
    "            COUNT(*) AS zero_close_count\n",
    "        FROM\n",
    "            tmp_pricing02_remsym_misshist\n",
    "        WHERE\n",
    "            close = 0\n",
    "        GROUP BY\n",
    "            act_symbol\n",
    "        HAVING\n",
    "            zero_close_count > 5\n",
    "        ),\n",
    "        zero_low_count AS (\n",
    "            SELECT\n",
    "                act_symbol,\n",
    "                COUNT(*) AS zero_low_count\n",
    "            FROM\n",
    "                tmp_pricing02_remsym_misshist\n",
    "            WHERE\n",
    "                low = 0\n",
    "            GROUP BY\n",
    "                act_symbol\n",
    "            HAVING\n",
    "                zero_low_count > 5\n",
    "        ),\n",
    "        zero_open_count AS (\n",
    "            SELECT\n",
    "                act_symbol,\n",
    "                COUNT(*) AS zero_open_count\n",
    "            FROM\n",
    "                tmp_pricing02_remsym_misshist\n",
    "            WHERE\n",
    "                open = 0\n",
    "            GROUP BY\n",
    "                act_symbol\n",
    "            HAVING\n",
    "                zero_open_count > 5\n",
    "        ),\n",
    "        zero_high_count AS (\n",
    "            SELECT\n",
    "                act_symbol,\n",
    "                COUNT(*) AS zero_high_count\n",
    "            FROM\n",
    "                tmp_pricing02_remsym_misshist\n",
    "            WHERE\n",
    "                high = 0\n",
    "            GROUP BY\n",
    "                act_symbol\n",
    "            HAVING\n",
    "                zero_high_count > 5\n",
    "        ),\n",
    "        zero_volume_count AS (\n",
    "            SELECT\n",
    "                act_symbol,\n",
    "                COUNT(*) AS zero_volume_count\n",
    "            FROM\n",
    "                tmp_pricing02_remsym_misshist\n",
    "            WHERE\n",
    "                volume = 0\n",
    "            GROUP BY\n",
    "                act_symbol\n",
    "            HAVING\n",
    "                zero_volume_count > 5\n",
    "        ),\n",
    "        filtered_symbols AS (\n",
    "            SELECT act_symbol FROM zero_close_count\n",
    "            UNION\n",
    "            SELECT act_symbol FROM zero_low_count\n",
    "            UNION\n",
    "            SELECT act_symbol FROM zero_open_count\n",
    "            UNION\n",
    "            SELECT act_symbol FROM zero_high_count\n",
    "            UNION\n",
    "            SELECT act_symbol FROM zero_volume_count\n",
    "        ),\n",
    "        remove_symbols_and_fix_zeros AS (\n",
    "            SELECT\n",
    "                date,\n",
    "                act_symbol,\n",
    "                COALESCE(\n",
    "                    NULLIF(open, 0),\n",
    "                    AVG(open) OVER (\n",
    "                        PARTITION BY act_symbol\n",
    "                        ORDER BY date asc\n",
    "                        ROWS BETWEEN 5 PRECEDING AND 1 PRECEDING\n",
    "                    )\n",
    "                ) AS open,\n",
    "                COALESCE(\n",
    "                    NULLIF(high, 0),\n",
    "                    MAX(high) OVER (\n",
    "                        PARTITION BY act_symbol\n",
    "                        ORDER BY date asc\n",
    "                        ROWS BETWEEN 5 PRECEDING AND 1 PRECEDING\n",
    "                    )\n",
    "                ) AS high,\n",
    "                COALESCE(\n",
    "                    NULLIF(low, 0),\n",
    "                    AVG(NULLIF(low, 0)) OVER (\n",
    "                        PARTITION BY act_symbol\n",
    "                        ORDER BY date asc\n",
    "                        ROWS BETWEEN 5 PRECEDING AND 1 PRECEDING\n",
    "                    )\n",
    "                ) AS low,\n",
    "                COALESCE(\n",
    "                    NULLIF(close, 0),\n",
    "                    AVG(close) OVER (\n",
    "                        PARTITION BY act_symbol\n",
    "                        ORDER BY date asc\n",
    "                        ROWS BETWEEN 5 PRECEDING AND 1 PRECEDING\n",
    "                    )\n",
    "                ) AS close,\n",
    "                COALESCE(\n",
    "                    NULLIF(volume, 0),\n",
    "                    AVG(NULLIF(volume, 0)) OVER (\n",
    "                        PARTITION BY act_symbol\n",
    "                        ORDER BY date asc\n",
    "                        ROWS BETWEEN 5 PRECEDING AND 1 PRECEDING\n",
    "                    )\n",
    "                ) AS volume\n",
    "            FROM\n",
    "                tmp_pricing02_remsym_misshist\n",
    "            WHERE\n",
    "                act_symbol NOT IN (SELECT act_symbol FROM filtered_symbols)\n",
    "        )\n",
    "        SELECT\n",
    "            *\n",
    "        FROM remove_symbols_and_fix_zeros\n",
    "        where high >= low\n",
    "    );\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "spark.sql(\"select * from tmp_pricing03_remsym_zeros\").display()\n",
    "\n",
    "spark.sql(f\"with cte as (select distinct act_symbol from tmp_pricing03_remsym_zeros) select count(*) as unique_act_symbol from cte\").display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "471d1959-e640-405b-bce7-94273d62f6f0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Resample to Business Day Frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c54ae43e-5016-43d8-9715-955300e2e0c9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.sql(\n",
    "\"\"\"\n",
    "CREATE OR REPLACE TEMPORARY VIEW tmp_pricing04_resampled AS (\n",
    "    -- Step 1: Identify the max date for each symbol\n",
    "    WITH max_dates AS (\n",
    "        SELECT\n",
    "            act_symbol,\n",
    "            MAX(date) AS max_date\n",
    "        FROM\n",
    "            tmp_pricing03_remsym_zeros\n",
    "        GROUP BY\n",
    "            act_symbol\n",
    "    ),\n",
    "\n",
    "    -- Step 2: Generate a business day calendar for the date range in your data\n",
    "    date_series AS (\n",
    "        SELECT\n",
    "            EXPLODE(SEQUENCE(\n",
    "                MIN(date), -- Start date of your data\n",
    "                MAX(date), -- End date of your data\n",
    "                INTERVAL 1 DAY\n",
    "            )) AS date\n",
    "        FROM\n",
    "            tmp_pricing03_remsym_zeros\n",
    "    ),\n",
    "    business_days AS (\n",
    "        SELECT\n",
    "            date\n",
    "        FROM\n",
    "            date_series\n",
    "        WHERE\n",
    "            EXTRACT(DOW FROM date) NOT IN (0, 6) -- Exclude Sundays (0) and Saturdays (6)\n",
    "        ORDER BY\n",
    "            date\n",
    "    ),\n",
    "\n",
    "    -- Step 3: Create a complete business day calendar for each symbol up to its max date\n",
    "    symbol_calendar AS (\n",
    "        SELECT\n",
    "            bd.date,\n",
    "            md.act_symbol\n",
    "        FROM\n",
    "            business_days bd\n",
    "        CROSS JOIN\n",
    "            max_dates md\n",
    "        WHERE\n",
    "            bd.date <= md.max_date\n",
    "    ),\n",
    "\n",
    "    -- Step 4: Join the calendar with your data and forward-fill missing values\n",
    "    resampled_data AS (\n",
    "        SELECT\n",
    "            sc.date,\n",
    "            sc.act_symbol,\n",
    "            COALESCE(p.open, LAST(p.open, TRUE) OVER (PARTITION BY sc.act_symbol ORDER BY sc.date ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW)) AS open,\n",
    "            COALESCE(p.high, LAST(p.high, TRUE) OVER (PARTITION BY sc.act_symbol ORDER BY sc.date ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW)) AS high,\n",
    "            COALESCE(p.low, LAST(p.low, TRUE) OVER (PARTITION BY sc.act_symbol ORDER BY sc.date ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW)) AS low,\n",
    "            COALESCE(p.close, LAST(p.close, TRUE) OVER (PARTITION BY sc.act_symbol ORDER BY sc.date ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW)) AS close,\n",
    "            COALESCE(p.volume, LAST(p.volume, TRUE) OVER (PARTITION BY sc.act_symbol ORDER BY sc.date ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW)) AS volume\n",
    "        FROM\n",
    "            symbol_calendar sc\n",
    "        LEFT JOIN\n",
    "            tmp_pricing03_remsym_zeros p\n",
    "        ON\n",
    "            sc.date = p.date AND sc.act_symbol = p.act_symbol\n",
    "    )\n",
    "\n",
    "    -- Final Output\n",
    "    SELECT * FROM resampled_data\n",
    "    WHERE open IS NOT NULL AND high IS NOT NULL AND low IS NOT NULL AND close IS NOT NULL AND volume IS NOT NULL\n",
    "    -- AND isnan(open) = false AND isnan(high) = false AND isnan(low) = false AND isnan(close) = false AND isnan(volume) = false\n",
    "    AND open != 'Infinity' AND high != 'Infinity' AND low != 'Infinity' AND close != 'Infinity' AND volume != 'Infinity'\n",
    "    ORDER BY act_symbol, date\n",
    ");\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "\n",
    "spark.sql(\"select * from tmp_pricing04_resampled\").display()\n",
    "\n",
    "spark.sql(f\"with cte as (select distinct act_symbol from tmp_pricing04_resampled) select count(*) as unique_act_symbol from cte\").display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c14afeae-f1b1-4bcb-b1c5-ef9ee5dba7da",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Remove Symbols with Insufficient History\n",
    "- Atleast 3 months / 63 trading days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5af35a3a-9c43-4a93-80ae-ec25219a35a2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.sql(\"\"\"\n",
    "CREATE OR REPLACE TEMPORARY VIEW tmp_pricing05_rem_sym_insuff_hist AS (\n",
    "    WITH symbol_counts AS (\n",
    "        SELECT \n",
    "            act_symbol, \n",
    "            COUNT(*) AS num_days \n",
    "        FROM tmp_pricing04_resampled\n",
    "        GROUP BY act_symbol\n",
    "    )\n",
    "    SELECT * FROM tmp_pricing04_resampled\n",
    "    WHERE act_symbol NOT IN (\n",
    "        SELECT act_symbol \n",
    "        FROM symbol_counts \n",
    "        WHERE num_days < 63\n",
    "    )\n",
    ");\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "\n",
    "spark.sql(\"select * from tmp_pricing05_rem_sym_insuff_hist\").display()\n",
    "\n",
    "spark.sql(f\"with cte as (select distinct act_symbol from tmp_pricing05_rem_sym_insuff_hist) select count(*) as unique_act_symbol from cte\").display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d25735b3-18a1-48a7-802f-f103424c2cc4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Create Pricing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3fa8f922-c866-4279-bf90-6f480ce1eb0b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.sql(f\"\"\"\n",
    "CREATE OR REPLACE TABLE featlib{catalog_suffix}.components.pricing AS (\n",
    "    SELECT * FROM tmp_pricing05_rem_sym_insuff_hist\n",
    ");\n",
    "\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "77e01179-3452-4d53-9f30-0bb9c76d6ffa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Data Quality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1c8dd4b1-3b37-48c4-9b56-3d90159ab845",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Unique symbols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3bb29527-58f8-4767-a85b-6763c650d9e2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.sql(f\"with cte as (select distinct act_symbol from featlib{catalog_suffix}.components.pricing) select count(*) as unique_act_symbol from cte;\").display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "115c00ef-1d11-458e-b4fb-992240fae433",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Weekend check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8de1fa01-a703-4dcd-9761-778a0416bc46",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.sql(f\"select * from raw{catalog_suffix}.stocks.ohlcv where date = '2025-01-11' limit 10;\").display()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "df311c14-0c42-44de-87fe-e87afb0e12fd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Check for nulls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7c9dcbbc-d7fb-4c92-b3f4-cf4afcd2c3e3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.sql(f\"\"\"\n",
    "SELECT * FROM featlib{catalog_suffix}.components.pricing\n",
    "    where open is null  or high is null or low is null or close is null or volume is null\n",
    "\"\"\").display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2b1f3515-7571-4198-86d6-299bd2141218",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Check for inf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6c98e315-7cd0-4a26-874d-4487643fbe92",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.sql(f\"\"\"\n",
    "SELECT * FROM featlib{catalog_suffix}.components.pricing\n",
    "    where open = 'Infinity' or high = 'Infinity' or low = 'Infinity' or close = 'Infinity' or volume = 'Infinity'\n",
    "\"\"\").display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "13f98fd0-5b47-4054-aba8-03c09891d15d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Test Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "10176679-fc42-4a96-b6df-b114624cc583",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Define symbol\n",
    "symbol = 'AAPL'\n",
    "\n",
    "# Query the data\n",
    "df = spark.sql(\n",
    "  f\"\"\"\n",
    "  SELECT \n",
    "      *,\n",
    "      log(close) as log_close\n",
    "  from featlib{catalog_suffix}.components.pricing a \n",
    "  where a.act_symbol = '{symbol}'\n",
    "  order by date asc\n",
    "  \"\"\"\n",
    ")\n",
    "\n",
    "df.display()\n",
    "\n",
    "# Plot the data using Plotly Express\n",
    "fig = px.line(df.toPandas(), x='date', y='close')\n",
    "fig.show()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 6299307711696471,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "pricing",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
