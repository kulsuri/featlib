{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "721494a4-b5c8-4bfe-b89e-eb46626aaafb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "56c7ad50-35a9-4c7f-9ecf-a7332797eb0d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append(\"/Workspace/Shared/lib/\")\n",
    "import os\n",
    "import logging\n",
    "import time\n",
    "import requests\n",
    "from datetime import datetime\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.utils import AnalysisException\n",
    "from params import get_env, get_catalog, get_schema, get_table, get_url, get_volume"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b0fa30bf-c46f-48ca-a379-ff372fab1ce7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "97a08a2f-175c-4b30-b7a6-1965afbec5b2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s %(name)s [%(levelname)s] %(message)s\",\n",
    "    stream=sys.stdout,\n",
    "    force=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "56b37cb5-37c2-4ba6-b870-eb107160300c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ff6ec848-639c-4b5a-bb60-aa5b11810abb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "env, catalog_suffix = get_env()\n",
    "catalog = get_catalog()\n",
    "schema = get_schema()\n",
    "table = get_table()\n",
    "url = get_url()\n",
    "volume = get_volume()\n",
    "\n",
    "print(f\"env = {env}\")\n",
    "print(f\"catalog_suffix = {catalog_suffix}\")\n",
    "print(f\"catalog = {catalog}\")\n",
    "print(f\"schema = {schema}\")\n",
    "print(f\"table = {table}\")\n",
    "print(f\"url = {url}\")\n",
    "print(f\"volume = {volume}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "92d587ec-4a26-4ca9-a6c3-90ac733c54ae",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "catalog = \"featlib_dev\"\n",
    "schema = \"components\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "33f5cfcf-1779-4fc6-91da-00812749a952",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Split Adjustment Factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8918bea6-ccc3-41ae-afb1-ddb7ba94e545",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.sql(\n",
    "    f\"\"\"\n",
    "    CREATE OR REPLACE TABLE {catalog}.{schema}.split_adj AS\n",
    "        -- Get the unique symbols from the split table\n",
    "        WITH base_table AS (\n",
    "            SELECT DISTINCT act_symbol FROM raw{catalog_suffix}.stocks.split\n",
    "        ),\n",
    "        -- Create a new table with a row for each symbol and a NULL split for date 2000-01-01\n",
    "        new_split_table AS (\n",
    "            SELECT \n",
    "                act_symbol,\n",
    "                DATE '2000-01-01' as ex_date,\n",
    "                NULL as to_factor,\n",
    "                NULL as for_factor\n",
    "            FROM base_table\n",
    "            UNION ALL\n",
    "            SELECT * FROM raw{catalog_suffix}.stocks.split\n",
    "        ),\n",
    "        -- Calculate the adjustment factor\n",
    "        split_factors AS (\n",
    "            -- Calculate adjustment factors and order the rows\n",
    "            SELECT\n",
    "                act_symbol,\n",
    "                ex_date AS adj_date,\n",
    "                try_divide(double(for_factor), double(to_factor)) AS adj_factor\n",
    "            FROM new_split_table\n",
    "        ),\n",
    "        -- Order the factors and assign end dates\n",
    "        ordered_factors AS (\n",
    "            -- Assign end dates and ensure chronological order\n",
    "            SELECT\n",
    "                act_symbol,\n",
    "                adj_date,\n",
    "                CAST (LEAD(adj_date) OVER (PARTITION BY act_symbol ORDER BY adj_date ASC) - INTERVAL '1 DAY' AS DATE) AS end_adj_date,\n",
    "                adj_factor,\n",
    "                ROW_NUMBER() OVER (PARTITION BY act_symbol ORDER BY adj_date DESC) AS reverse_row_num\n",
    "            FROM split_factors\n",
    "        ),\n",
    "        -- Calculate the cumulative adjustment factor\n",
    "        cumulative_factors AS (\n",
    "            -- Compute cumulative adjustment factors in reverse order\n",
    "            SELECT\n",
    "                act_symbol,\n",
    "                adj_date,\n",
    "                end_adj_date,\n",
    "                adj_factor,\n",
    "                -- Reverse chronological cumulative multiplication\n",
    "                CASE \n",
    "                    WHEN end_adj_date IS NULL \n",
    "                        THEN 1\n",
    "                    WHEN lead(end_adj_date) OVER (PARTITION BY act_symbol ORDER BY adj_date ASC) IS NULL\n",
    "                        THEN lead(adj_factor) OVER (PARTITION BY act_symbol ORDER BY adj_date ASC)\n",
    "                    --ELSE product(adj_factor) OVER (PARTITION BY act_symbol ORDER BY reverse_row_num ASC ROWS BETWEEN UNBOUNDED PRECEDING AND 1 PRECEDING)\n",
    "                    ELSE EXP(SUM(LN(adj_factor)) OVER (PARTITION BY act_symbol ORDER BY reverse_row_num ASC ROWS BETWEEN UNBOUNDED PRECEDING AND 1 PRECEDING))\n",
    "                END AS cum_adj_factor\n",
    "            FROM ordered_factors\n",
    "        )\n",
    "        -- Final output\n",
    "        SELECT\n",
    "            act_symbol,\n",
    "            adj_date,\n",
    "            end_adj_date,\n",
    "            adj_factor,\n",
    "            cum_adj_factor\n",
    "        FROM cumulative_factors\n",
    "        ORDER BY adj_date ASC;\n",
    "    \"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "20709e6a-ea1c-42c1-9730-312a05a0d25c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Pricing Inc. Splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "73f4dd1e-44c5-4b7b-997c-9e7973657eff",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.sql(\n",
    "    f\"\"\"\n",
    "    -- 17457\n",
    "    CREATE OR REPLACE TEMPORARY VIEW tmp_pricing01_inc_splits AS (\n",
    "        with ohlcv_split as (\n",
    "            select\n",
    "                a.date,\n",
    "                a.act_symbol,\n",
    "                a.open * COALESCE(f.cum_adj_factor, 1) as open,\n",
    "                a.high * COALESCE(f.cum_adj_factor, 1) as high,\n",
    "                a.low * COALESCE(f.cum_adj_factor, 1) as low,\n",
    "                a.close * COALESCE(f.cum_adj_factor, 1) as close,\n",
    "                a.volume / COALESCE(f.cum_adj_factor, 1) as volume\n",
    "            FROM raw{catalog_suffix}.stocks.ohlcv a\n",
    "            LEFT JOIN featlib{catalog_suffix}.components.split_adj f \n",
    "                ON a.act_symbol=f.act_symbol \n",
    "                AND a.date >= f.adj_date \n",
    "                AND (a.date <= f.end_adj_date OR f.end_adj_date IS NULL)\n",
    "        )\n",
    "        select * from ohlcv_split\n",
    "    )\n",
    "    \"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "137da9ee-4299-48e2-91de-ada963515656",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "with cte as (select distinct act_symbol from tmp_pricing01_inc_splits) select count(*) from cte"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "821aec5a-f5fc-424d-b60d-6dd537c8c635",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Remove Symbols With History Gaps\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 8750531619292766,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "pricing",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
